# K8S集群网络

## 1、使用命令跟踪网络包

https://zhuanlan.zhihu.com/p/558319921

## 2、docker网桥原理

https://zhuanlan.zhihu.com/p/558785823

## 2、K8s 网络模型

Kubernetes 要求所有的网络插件实现必须满足如下要求：

- 一个Pod一个IP
- 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射
- 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射
- Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个。

> NAT（Network Address Translation，网络地址转换）映射是一种网络技术，用于将私有网络内部的IP地址与公共网络之间的IP地址进行转换和映射。
>
> 这种技术通常用于家庭网络、企业网络和互联网服务提供商（ISP）网络中，以帮助多个设备共享单个公共IP地址或者将内部网络隐藏在一个或多个公共IP地址后面。
>
> NAT映射的主要目的是解决IPv4地址枯竭问题，同时提供一定程度的网络安全性。

## 3、K8s 网络四层结构

<img src="https://pic1.zhimg.com/80/v2-28cd36ef5187ad07d5e95155f6b5658c_1440w.webp" alt="img" style="zoom: 67%;" />

1. 第0层Node网络（NodeIP + Port）：保证k8s集群中的节点之间能够正常做IP寻址和互通的一个网络，这个一般是由底层的网络基础设施组成的，比如公有云或自建的数据中心。

2. 第1层Pod网络（PodIP + Port）：保证k8s集群当中所有的pod，包括同一节点上的pod和不同节点上的pod逻辑上看起来都在同一个平面网络内，能够相互做ip寻址并且相互通信的网络。

3. 第2层Service网络（Cluster IP +Port）

4. 第3层外部接入网络（NodePort、LoadBalancer、Ingress）

   ![img](https://pic1.zhimg.com/80/v2-1215cef031b2b52e29696be6942fbd5c_1440w.webp)

## 4、Pod网络

### 4.1、同一Node的pod网络

<img src="https://pic1.zhimg.com/80/v2-1b98639bf4ffe839a3ed29ecce8143ec_1440w.webp" alt="img" style="zoom:50%;" />

上图展示了pod网络所依赖的3个网络设备：

1. **eth0**，这个是Node主机上的网卡，它是支持节点流量出入的一个设备，也支持k8s集群节点中间做ip寻址和互通的设备。
   1. Node内的pod网络在172.17.0.0/24这个地址空间内，但是Node的主机在10.100.0.0/24这个地址空间内，也就是说pod网络和Node网络不在同一个网络内。
2. **docker0**虚拟网桥，简单可以理解为一个虚拟交换机，实现了Node上面的**pod之间互联**。
   1. **pod1的ip是由docker0网桥分配的**，比如docker0网桥是172.17.0.1给第一个pod分配了一个ip是172.17.0.2，如果这个Node上面再启一个pod2，那么这个网桥会相应的给pod2分配ip 172.17.0.3，如果再启pod就可以以此类推，因为这些pod都是连在同一个网桥上面的，在同一个网段内，它们可以进行ip寻址和互通。
   2. **每个Node中有一个虚拟网桥，多个虚拟网卡，因为一个网卡对应一个Pod。**
3. **veth0**，是pod1的虚拟网卡，支持pod内容器互通并且相互访问的一个虚拟设备。
   1. pod1内部有三个容器，都共享这个虚拟网卡veth0，内部的这些容器可以通过localhost相互访问这就是**共享网络栈**的意思。注意pod内部的容器不能在同一个端口上同时开启服务，否则会有端口冲突问题。
   2. pod1当中还有一个比较特殊的容器叫pause，这个容器运行的唯一目的是为pod建立共享的veth0网络接口（共享网络栈），如果ssh到k8s集群当中一个有pod运行的节点上面去运行docker ps就可以看到pause这个容器。



补充资料：

> 1. 根据命令查看实际的通信流程：
>
> https://zhuanlan.zhihu.com/p/603605839
>
> 2. 通过命令查看Pod的容器如何共享命名空间？
>
>    https://zhuanlan.zhihu.com/p/603545786



### 4.2、不同Node的Pod网络

<img src="https://pic1.zhimg.com/80/v2-25159146b7da66431d921639ee2cea08_1440w.webp" alt="img" style="zoom:67%;" />

假设有2个节点主机，一个是host1，ip为10.100.0.2 ，另外host2，ip是10.100.0.3，2个节点都在10.100.0.0/24这个地址空间内，host1上有一个pod x ，ip是172.17.0.2，pod2上有一个pod y，它的ip是172.17.1.3，

Node网络和pod网络不在同一个地址空间内，host1上的pod x是如何与host2上的pod y进行互通的？

不同节点上的pod互通有很多种技术实现方案，底层的技术细节也很复杂，大体分为2类，一类是路由，另一类是覆盖网络方案。

1. 路由

   <img src="https://pic2.zhimg.com/80/v2-aff0db8e7f34c37747218c4404029ff1_1440w.webp" alt="img" style="zoom:67%;" />

   原理：通过路由设备为k8s集群的pod网络单独的划分网段，并且配置路由器支持pod网络的转发。

   比如对于目标为172.17.1.0/24这个范围内的包转发到10.100.0.3这个主机上，同样对于目标为172.17.0.0/24这个范围内的包转发到10.100.0.2这个主机上。当主机上的eth0接受到来自pod网络的包就会向内部的pod网桥进行转发，这样不同节点之间的pod就可以做相互的ip寻址和通讯。

   2. 覆盖网络（overlay方案，**第三方网络插件方案**）

      <img src="https://pic2.zhimg.com/80/v2-56b421b476c51ea2db176fda1216c099_1440w.webp" alt="img" style="zoom:50%;" />

      原理：在现有网络的基础上再建立一个虚拟网络，实现的技术有很多比如flannel，这些方案大都采用隧道封包的技术（pod网络的数据包在出节点之前会先被封装成节点网络的数据包，当数据包经过底层网络到达目标节点，这个pod网络数据包就会被解封出来，再转发给内部的pod网络）。

      ![img](https://pic4.zhimg.com/80/v2-13dd3dfc9d354f7409cb843a21b429e7_1440w.webp)

      如上图，每个node上的pod ip和cni0网桥ip和flannel ip都是在同一个网段10.1.71.x上。

      - flannel怎么划分网段？flannel将整个k8s的node网络地址分成一块一块的。每个node的网段是互相不重叠的。flannel network是10.1.0.0，10.1是整个集群总的网段，给每个node使用ip地址的第三位即给每个node再划分网段，每个node再用具体划分的网段再给pod分配ip地址。
      - 每个node上的都有一个flannel，这个flannel使用的网段就是当前node的网段。

      - pod和pod之间的通信，如果是在同一个node上，就使用cni这个网桥进行报文的转发；如果去连接别的node上的pod，那么就会通过flannel网络设备。
      - flannel使用的是vxlan（虚拟扩展网络）。

      >相关命令：参考资料：https://zhuanlan.zhihu.com/p/603642190
      >
      >1. 怎么查看每个node的网段？get node -o wide

      

### 4.3、CNI

<img src="https://pic3.zhimg.com/80/v2-cbe0ca60cee8c3503d7c0035ca883726_1440w.webp" alt="img" style="zoom:50%;" />

考虑到pod网络实现技术众多，为了简化集成，k8s支持CNI（容器网络接口）这样一个标准，不同的网络技术可以通过**CNI**以插件的形式和k8s进行集成，k8s当中的kubelet通过CNI接口去操作pod网络比如删除或添加网络接口都通过CNI这个plugin操作，就可以做到不需要关心这个pod网络的底层的具体实现细节。

## 5、Service 网络

有了pod网络，k8s集群当中的所有的pod在逻辑上都可以看做在一个平面网络内可以正常的做ip寻址和互通，但是这个pod仅仅是k8s云平台当中的虚拟机抽象，最终我们需要在k8s集群当中运行的是应用或者服务service，

<u>**而一个service背后一般是多个pod组成的集群**</u>，这个时候就引入了服务发现discover和负载均衡load balancer，这些问题就是第二层service网络要解决的问题。

K8s的Service网络构建在Pod网络之上，主要目标是解决服务发现和负载均衡。

### 5.1 Service网络的概念模型

<img src="https://pic2.zhimg.com/80/v2-e7a224d0be8b0d7aa0d1e4968410f55d_1440w.webp" alt="img" style="zoom:50%;" />

Account-App是4个pod组成的集群一起提供服务，每个pod都有自己的pod ip和端口号，再假定集群内还部署了其他的应用，这些应用有些是account-App的消费方，也就说有些client pod要访问Account Service，这个时候就自然引入2个问题：

1. 服务发现。client pod如何发现并且定位account app集群中的pod ip，况且account app集群中的ip有可能会变（逾期的情况，比如重新发布；非逾期的情况， 比如pod挂了，k8s重新调度部署，pod ip也会变化）
2. client pod要以某种负载均衡策略访问Account-App集群中不同的pod实例，来实现负载分摊和高可用的访问

解决方案：

1. Cluster IP来解决服务发现问题，client不关心pod集群中具体的pod数量和pod ip，即使pod ip发生变化也会被Cluster IP所屏蔽（注意这里的Cluster IP是虚拟ip），支持以不同的策略去访问pod集群中的不同的pod实例以实现负载分摊还有HA高可用。
2. k8s当中默认的负载均衡策略是Round-Robin（轮询调度算法的原理是每一次把来自用户的请求轮流分配给内部中的服务器，从1开始，直到N(内部服务器个数)，然后重新开始循环），也可以采用其他的复杂的负载均衡策略。

### 5.2 Service网络具体的实现原理

<img src="https://pic1.zhimg.com/80/v2-43d73161d01942320924711619decf40_1440w.webp" alt="img" style="zoom:50%;" />

在k8s平台的每个worker节点上面都部署2个组件（kubelet和kube-proxy），这两个组件和master是实现Service网络的关键。

1. 服务注册：pod实例发布的时候 ，kubelet会负责启动这些pod实例，启动完之后，kubelet会把服务的pod ip列表汇报注册到master节点上。service发布时，k8s会为service分配相应的cluster ip，相关的信息也会记录在cluster上面，并且cluster ip和pod ip是有映射关系的。

2. 服务发现：kube-proxy会监听master，并且发现服务的cluster ip和pod之间的映射的列表，并且修改本地的`linux iptables`的转发规则（接受到某个cluser ip请求的时候进行负载均衡并且转发到对应的pod ip）。
3. 服务实际调用过程（service name + cluster ip）：当有消费者pod需要访问某个服务的时候，就通过cluser ip发请求，这个cluser ip调用会被本地的iptables所截获，然后通过负载均衡转发到目标服务的这些pod实例上面。实际消费者pod也不是直接调用服务的cluster ip的，而是先调用服务名，因为cluster ip也可能会变化，比如针对不同的环境下（test环境、uat环境），只有服务名称一般是不变的，为了屏蔽cluster ip的变化，k8s在每个worker节点上面还有一个`Kube-DNS`组件，它也监听master节点并且发现cluster ip和服务名它们之间的映射关系，这样消费者pod通过Kube-DNS可以间接的发现cluster ip，然后cluster ip就可以通过iptables转发到这些目标pod上面去。

### Service实现原理



[![1210730-20191114110528617-401258978](https://img2020.cnblogs.com/blog/1616600/202105/1616600-20210504223446159-1526200665.png)](https://img2020.cnblogs.com/blog/1616600/202105/1616600-20210504223446159-1526200665.png)

`Service`会通过`API Server`持续监视着（`watch`）标签选择器（label-selector）匹配到的后端`Pod`对象，并实时跟踪各对象的变动，例如，`IP`地址变动、对象增加或减少等。

`Service`并不直接链接至`Pod`对象，它们之间还有一个中间层——`Endpoints`资源对象，它是一个由`IP`地址和端口组成的列表，这些`IP`地址和端口则来自由`Service`的标签选择器匹配到的`Pod`资源。当创建`service`对象时，其关联的`Endpoints`对象会自动创建。

一个`Service`对象就是工作节点上的一些`iptables`或`ipvs`规则，用于将到达`Service`对象`IP`地址的流量调度转发至相应的`Endpoints`对象指定的`IP`地址和端口之上。`kube-proxy`组件通过`API Server`持续监控着各`Service`及其关联的`Pod`对象，并将其创建或变动实时反映到当前工作节点上的`iptables`规则或`ipvs`规则上。

`ipvs`是借助于`Netfilter`实现的网络请求报文调度框架，支持`rr`、`wrr`、`lc`、`wlc`、`sh`、`sed`和`nq`等十余种调度算法，用户空间的命令行工具是`ipvsadm`，用于管理工作与`ipvs`之上的调度规则。

`Service IP`事实上是用于生成`iptables`或`ipvs`规则时使用的`IP`地址，仅用于实现`Kubernetes`集群网络的内部通信，并且能够将规则中定义的转发服务的请求作为目标地址予以相应，这也是将其称为虚拟`IP`的原因之一。

`kube-proxy`将请求代理至相应端点的方式有三种：**userspace（用户空间）**、**iptables**和**ipvs**。因userspace传输效率太低，不推荐使用，在1.1版本之前是默认转发策略，现在默认是iptables。

### Service三种类型

`Service`的`IP`地址只能够在集群内部可访问，对一些应用（如`frontend`）的某些部分，可能希望通过外部（`kubernetes`集群外部）`IP`地址暴露`Service`，这时候就需要使用到`NodePort`。`kubernetes ServiceTypes`支持四种类型：`ClusterIP`、`NodePort`、`LoadBalancer`、其默认是`Cluster IP`类型。

- **ClusterIP**：默认策略，分配一个稳定的IP地址，即VIP，只能在集群内部访问（同Namespace内的Pod）。 `pod ---> ClusterIP:ServicePort --> (iptables、ipvs)DNAT --> PodIP:containePort`

- **NodePort**：在每个节点上启用一个端口来暴露服务，可以在集群外部访问。也会分配一个稳定内部集群IP地址。

  访问地址：<NodeIP>:<NodePort> `clietn --> <NodeIp>:<NodePort> --> (iptables、ipvs)DNAT --> <PodIP>:<ContainerPort>`。

- **LoadBalancer**：与NodePort类似，在每个节点上启用一个端口来暴露服务。除此之外，Kubernetes会请求底层云平台上的负载均衡器，将每个Node（[NodeIP]:[NodePort]）作为后端添加进去。

### 5.4 NodePort

```text
Service 应用是K8s集群内部可见的
而我们发布的应用需要外网甚至公网可以访问
K8s如何将内部服务暴露出去？
四层网络只有Node节点网络可以对外通讯
现在问题是第2层的Service网络如何通过第0层Node节点网络暴露出去呢？
需要再思考一个问题 在k8s服务发现原理图中 哪个组件既知道
service网络的所有信息又可以和pod网络互通互联同时又可以与节点网络打通呢？
那就是Kube-Proxy 对外暴露服务也是通过这个组件实现的

只需要让Kube-Proxy在节点上暴露一个监听端口就可以了 
所以NodePort就闪亮登场了
```

<img src="https://pic2.zhimg.com/80/v2-3568abbbe690fcf678b059effea1c5b5_1440w.webp" alt="img" style="zoom:67%;" />

将service type设置为NodePort 端口范围在30000～32767之间，k8s发布以后 会在每个节点上都会开通NodePort端口，这个端口的背后就是Kube-Proxy。

当外部流量想要访问k8s服务的时候，先访问NodePort端口 然后通过Kube-Proxy转发到内部的Service抽象层，然后再转发到目标Pod上去。

### 5.5 LoadBalancer

<img src="https://pic1.zhimg.com/80/v2-a626c44049fe32e8c0aa19140fdd3e14_1440w.webp" alt="img" style="zoom: 67%;" />

如果在阿里云上有一套k8s环境，将service type设置为LoadBalancer，阿里云K8s会自动创建NodePort进行端口转发，同时也会
申请一个SLB 有独立的公网IP，并且也会自动映射K8s集群的NodePort上，以上是生产环境 就可以通过SLB暴露出去的公网IP访问到K8s集群内部的NodePort。
但在开发测试环境可以直接通过NodePort访问

这种方式的劣势：

如果暴露一个服务就需要购买一个LB+IP
如果暴露10个服务就需要购买10个LB+IP
所以成本比较高
那有没有办法购买一个LB+IP能不能将更多的服务暴露出去呢？
那么Ingress就闪亮登场了
也就是在K8s内部部署一个独立的反向代理服务 让它做代理转发

### 5.6 Ingress

参考文档：https://zhuanlan.zhihu.com/p/604661179

![img](https://pic1.zhimg.com/80/v2-94db3703b2503fed37a79c15bf1134e4_1440w.webp)

```text
Ingress是一个特殊的service 通过节点80/443暴露出去
Ingress可以通过path或者域名转发到Service抽象层然后转发到Pod
只需要设置好转发的路由表即可
本质上和Nginx没有差别
service kind设置为ingress
ingress提供的主要功能是七层反向代理 如果暴露的是四层服务还是需要走LB+IP方式
还可以做安全认证、监控、限流、证书等高级功能
有了Ingress就可以购买一个LB+IP就可以将k8s集群中的多个service暴露出来
```

## 6、Kube-Proxy

<img src="https://pic4.zhimg.com/80/v2-d8af2acc4f8de9b6fbe5a050afd2e2df_1440w.webp" alt="img" style="zoom:50%;" />

Kube-Proxy主要实现服务发现和负载均衡以及ClusterIP到PodIP的转换

Kube-Proxy通过linux内核提供的2个机制（Netfilter和iptables）间接实现，通过这2个机制的配合来实现IP地址的转换以及流量的路由。

- Netfilter是linux内核支持的一种钩子方法，允许内核的其他模块注册回调方法，这些回调方法可以截获网络包，可以改变它们的目的地路由。
- iptables是一组用户空间程序，通过它可以设置Netfilter中的路由规则，iptables程序可以检查、转发、修改、重定向或者丢弃ip网络包，iptables是Netfiter用户空间接口，可以间接操作Netfilter中的路由规则。

Kube-Proxy可以通过iptabels程序可以去操作内核空间的Netfilter里面的路由规则，而Netfilter可以截获底层的IP网络包就可以修改它们的路由。

参考kube-proxy文档：https://zhuanlan.zhihu.com/p/420782828

### Iptables原理

![1210730-20191114110743774-650618117](https://img2020.cnblogs.com/blog/1616600/202105/1616600-20210504223446700-675302199.png)

> `iptables`代理模式中，`kube-proxy`负责跟踪`API Server`上`Service`和`Endpoints`对象的变动（创建或移除），并据此作出`Service`资源定义的变动。同时，对于每个`Service`对象，它都会创建`iptables`规则直接捕获到达`Cluster IP`（虚拟IP）和`Port`的流量，并将其重定向至当前`Service`的后端。对于每个`Endpoints`对象，`Service`资源会为其创建`iptables`规则并关联至挑选的后端`Pod`资源，默认的调度算法是随机调度（`random`）。实现基于客户端`IP`的会话亲和性（来自同一个用户的请求始终调度到后端固定的一个`Pod`），可将`service.spec.sessionAffinity`的值设置为`“ClientIP”`（默认值为`“None”`）。

其代理过程是：请求到达`service`后，其请求被相关`service`上的`iptables`规则进行调度和目标地址转换（`DNAT`）后再转发至集群内的`Pod`对象之上。

ptables：

- 灵活，功能强大
- 规则遍历匹配和更新，呈线性时延

### IPVS原理

![1210730-20191114110851477-405128827](https://img2020.cnblogs.com/blog/1616600/202105/1616600-20210504223447397-1791805925.png)

`kube-proxy`跟踪`API Server`上`Service`的`Endpoints`对象的变动，据此来调用`netlink`接口创建`ipvs`规则，并确保与`API Server`中的变动保持同步，其请求流量的调度功能由`ipvs`实现，其余的功能由`iptables`实现。`ipvs`支持众多调度算法，如`rr`、`lc`、`dh`、`sh`、`sed`和`nq`等。

IPVS：

- 工作在内核态，有更好的性能
- 调度算法丰富：rr，wrr，lc，wlc，ip hash...

### 将service代理模式改为IPVS

https://www.cnblogs.com/LiuChang-blog/p/14730486.html







## 1、Docker容器网络模型

先看下Linux网络名词：

- **网络的命名空间：**Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；Docker利用这一特性，实现不同容器间的网络隔离。

- **Veth设备对：**Veth设备对的引入是为了实现在不同网络命名空间的通信。

- **Iptables/Netfilter：**Docker使用Netfilter实现容器网络转发。

- **网桥：**网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来，并实现类似交换机那样的多对多的通信。

- **路由：**Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里。

Docker容器网络示意图如下：

![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/docker-network.png)



### 2、Pod 网络

**问题：**Pod是K8S最小调度单元，一个Pod由一个容器或多个容器组成，当多个容器时，怎么都用这一个Pod IP？

**实现：**k8s会在每个Pod里先启动一个infra container小容器，然后让其他的容器连接进来这个网络命名空间，然后其他容器看到的网络试图就完全一样了。即网络设备、IP地址、Mac地址等。这就是解决网络共享的一种解法。在Pod的IP地址就是infra container的IP地址。

![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/c-to-c.png)



在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。

Pod之间通信会有两种情况：

- 两个Pod在同一个Node上
- 两个Pod在不同Node上

**先看下第一种情况：两个Pod在同一个Node上**

同节点Pod之间通信道理与Docker网络一样的，如下图：

![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/pod-to-pod-2.gif)

1. 对 Pod1 来说，eth0 通过虚拟以太网设备（veth0）连接到 root namespace；
2. 网桥 cbr0 中为 veth0 配置了一个网段。一旦数据包到达网桥，网桥使用ARP 协议解析出其正确的目标网段 veth1；
3. 网桥 cbr0 将数据包发送到 veth1；
4. 数据包到达 veth1 时，被直接转发到 Pod2 的 network namespace 中的 eth0 网络设备。

**再看下第二种情况：两个Pod在不同Node上**

K8S网络模型要求Pod IP在整个网络中都可访问，这种需求是由第三方网络组件实现。



### 3、CNI（容器网络接口）

CNI（Container Network Interface，容器网络接口)：是一个容器网络规范，Kubernetes网络采用的就是这个CNI规范，CNI实现依赖两种插件，一种CNI Plugin是负责容器连接到主机，另一种是IPAM负责配置容器网络命名空间的网络。

CNI插件默认路径：

```
# ls /opt/cni/bin/
```

地址：https://github.com/containernetworking/cni

当你在宿主机上部署Flanneld后，flanneld 启动后会在每台宿主机上生成它对应的CNI 配置文件（它其实是一个 ConfigMap），从而告诉Kubernetes，这个集群要使用 Flannel 作为容器网络方案。

CNI配置文件路径：

```
/etc/cni/net.d/10-flannel.conflist
```

当 kubelet 组件需要创建 Pod 的时候，先调用dockershim它先创建一个 Infra 容器。然后调用 CNI 插件为 Infra 容器配置网络。

这两个路径在kubelet启动参数中定义： 

```
 --network-plugin=cni \
 --cni-conf-dir=/etc/cni/net.d \
 --cni-bin-dir=/opt/cni/bin
```

## 4.3 Kubernetes网络组件之 Flannel

Flannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。

### 1、Flannel 部署

 https://github.com/coreos/flannel 

```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

### 2、Flannel工作模式及原理

Flannel支持多种数据转发方式：

- UDP：最早支持的一种方式，由于性能最差，目前已经弃用。
- VXLAN：Overlay Network方案，源数据包封装在另一种网络包里面进行路由转发和通信
- Host-GW：Flannel通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机都有整个容器网络的路由数据了。

### 3、Flannel VXLAN 隧道模式

```
# kubeadm部署指定Pod网段
kubeadm init --pod-network-cidr=10.244.0.0/16

# 二进制部署指定
cat /opt/kubernetes/cfg/kube-controller-manager.conf
--allocate-node-cidrs=true \
--cluster-cidr=10.244.0.0/16 \
```



```
# kube-flannel.yml
net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
```



vxlan原理：

https://zhuanlan.zhihu.com/p/559248026

https://zhuanlan.zhihu.com/p/559421839

为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。下图flannel.1的设备就是VXLAN所需的VTEP设备。示意图如下：

![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/flanneld-vxlan.png)



如果Pod 1访问Pod 2，源地址10.244.1.10，目的地址10.244.2.10 ，数据包传输流程如下：

1. **容器路由：**容器根据路由表从eth0发出

   ```
   # ip route
   default via 10.244.0.1 dev eth0 
   10.244.0.0/24 dev eth0 scope link  src 10.244.0.45 
   10.244.0.0/16 via 10.244.0.1 dev eth0 
   ```

 2. **主机路由：**数据包进入到宿主机虚拟网卡cni0，根据路由表转发到flannel.1虚拟网卡，也就是，来到了隧道的入口。

    ```
    # ip route
    default via 192.168.31.1 dev ens33 proto static metric 100 
    10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 
    10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink 
    10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink 
    ```

  3. **VXLAN封装：**而这些VTEP设备（二层）之间组成二层网络必须要知道目的MAC地址。这个MAC地址从哪获取到呢？

     其实在flanneld进程启动后，就会自动添加其他节点ARP记录，可以通过命令查看，如下所示：

     ```
     # ip neigh show dev flannel.1
     10.244.1.0 lladdr ca:2a:a4:59:b6:55 PERMANENT
     10.244.2.0 lladdr d2:d0:1b:a7:a9:cd PERMANENT
     ```

4. **二次封包：**知道了目的MAC地址，封装二层数据帧（容器源IP和目的IP）后，对于宿主机网络来说这个帧并没有什么实际意义。接下来，Linux内核还要把这个数据帧进一步封装成为宿主机网络的一个普通数据帧，好让它载着内部数据帧，通过宿主机的eth0网卡进行传输。

   ![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/vxlan-pkg.png)

5. **封装到UDP包发出去：**现在能直接发UDP包嘛？到目前为止，我们只知道另一端的flannel.1设备的MAC地址，却不知道对应的宿主机IP地址是什么。

   flanneld进程也维护着一个叫做FDB的转发数据库，可以通过`bridge fdb`命令查看：

   ```
   # bridge fdb show  dev flannel.1
   d2:d0:1b:a7:a9:cd dst 192.168.31.61 self permanent
   ca:2a:a4:59:b6:55 dst 192.168.31.63 self permanent
   ```

   可以看到，上面用的对方flannel.1的MAC地址对应宿主机IP，也就是UDP要发往的目的地。然后使用这个目的IP进行封装。

6. **数据包到达目的宿主机后：**Node2的eth0网卡发现是VXLAN数据包，把它交给flannel.1设备。flannel.1设备则会进一步拆包，取出原始二层数据帧包，发送ARP请求，经由cni0网桥转发给container。

### 4、Flannel Host-GW 路由模式

host-gw模式相比vxlan简单了许多， 直接使用路由表，将目的主机当做网关，直接路由原始封包。 

下面是示意图：

![](https://k8s-1252881505.cos.ap-beijing.myqcloud.com/k8s-2/flanneld-hostgw.png)

```
# kube-flannel.yml

net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "host-gw"
      }
    }
```

当你设置flannel使用host-gw模式,flanneld会在宿主机上创建节点的路由表：

```
# ip route

default via 192.168.31.1 dev ens33 proto static metric 100 
10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 
10.244.1.0/24 via 192.168.31.63 dev ens33 
10.244.2.0/24 via 192.168.31.61 dev ens33 
192.168.31.0/24 dev ens33 proto kernel scope link src 192.168.31.62 metric 100 
```

目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址是 192.168.31.63（即：via 192.168.31.63）。

一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。

而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.20，即 container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 container-2 当中。

## 4.3 Kubernetes网络组件之 Calico

原理：https://zhuanlan.zhihu.com/p/560261915

### 1、Calico介绍

在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现容器的网络，这些二层的技术有一些弊端，比如需要依赖 VLAN、bridge 和隧道等技术，其中 bridge 带来了复杂性，vlan 隔离和 tunnel 隧道则消耗更多的资源并对物理环境有要求，随着网络规模的增大，整体会变得越加复杂。

我们尝试把 Host 当作 Internet 中的路由器，同样使用 BGP 同步路由，并使用 iptables 来做安全访问策略，最终设计出了 Calico 方案。

**设计思想**：Calico 不使用隧道或 NAT 来实现转发，而是巧妙的把所有二三层流量转换成三层流量，并通过 host 上路由配置完成跨 Host 转发。

**设计优势**：

1.更优的资源利用

二层网络通讯需要依赖广播消息机制，广播消息的开销与 host 的数量呈指数级增长，Calico 使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。

另外，二层网络使用 VLAN 隔离技术，天生有 4096 个规格限制，即便可以使用 vxlan 解决，但 vxlan 又带来了隧道开销的新问题。而 Calico 不使用 vlan 或 vxlan 技术，使资源利用率更高。

2.可扩展性

Calico 使用与 Internet 类似的方案，Internet 的网络比任何数据中心都大，Calico 同样天然具有可扩展性。

3.简单而更容易 debug

因为没有隧道，意味着 workloads 之间路径更短更简单，配置更少，在 host 上更容易进行 debug 调试。

4.更少的依赖

Calico 仅依赖三层路由可达。

5.可适配性

Calico 较少的依赖性使它能适配所有 VM、Container、白盒或者混合环境场景。

### Calico 架构

![](https://img2018.cnblogs.com/blog/1060878/201904/1060878-20190413152300545-538840176.png)

Calico网络模型主要工作组件：

1.Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。

2.etcd：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；

3.BGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。

4.BGP Route Reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。

 

**Felix**

Felix会监听ECTD中心的存储，从它获取事件，比如说用户在这台机器上加了一个IP，或者是创建了一个容器等。用户创建pod后，Felix负责将其网卡、IP、MAC都设置好，然后在内核的路由表里面写一条，注明这个IP应该到这张网卡。同样如果用户制定了隔离策略，Felix同样会将该策略创建到ACL中，以实现隔离。

 

**BIRD**

BIRD是一个标准的路由程序，它会从内核里面获取哪一些IP的路由发生了变化，然后通过标准BGP的路由协议扩散到整个其他的宿主机上，让外界都知道这个IP在这里，你们路由的时候得到这里来。

 

**架构特点**

由于Calico是一种纯三层的实现，因此可以避免与二层方案相关的数据包封装的操作，中间没有任何的NAT，没有任何的overlay，所以它的转发效率可能是所有方案中最高的，因为它的包直接走原生TCP/IP的协议栈，它的隔离也因为这个栈而变得好做。因为TCP/IP的协议栈提供了一整套的防火墙的规则，所以它可以通过IPTABLES的规则达到比较复杂的隔离逻辑。

### 2、Calico 两种模式

1. IPIP

从字面来理解，就是把一个IP数据包又套在一个IP包里，即把 IP 层封装到 IP 层的一个 tunnel。它的作用其实基本上就相当于一个基于IP层的网桥！一般来说，普通的网桥是基于mac层的，根本不需 IP，而这个 ipip 则是通过两端的路由做一个 tunnel，把两个本来不通的网络通过点对点连接起来。 

2. BGP

边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或‘前缀’表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。BGP，通俗的讲就是讲接入到机房的多条线路（如电信、联通、移动等）融合为一体，实现多线单IP，BGP 机房的优点：服务器只需要设置一个IP地址，最佳访问路由是由网络上的骨干路由器根据路由跳数与其它技术指标来确定的，不会占用服务器的任何系统。

### 2、Calico BGP 路由模式

https://www.cnblogs.com/goldsunshine/p/10701242.html

![](https://img2018.cnblogs.com/blog/1060878/201904/1060878-20190413150713064-1083686463.png)

第一条路由的意思是：去往任何网段的数据包都发往网关169.254.1.1，然后从eth0网卡发送出去。

> 路由表中Flags标志的含义：
>
> U up表示当前为启动状态
>
> H host表示该路由为一个主机，多为达到数据包的路由
>
> G Gateway 表示该路由是一个网关，如果没有说明目的地是直连的
>
> D Dynamicaly 表示该路由是重定向报文修改
>
> M 表示该路由已被重定向报文修改



### 3、IPIP 模式

### 4、Route Relector（RR）路由反射方案优化网络

### 5、办公网络与K8s内部网络互通方案

### 6、CNI网络方案的优缺点及选择



## 4.5 网络策略（Pod ACL）

### 1、网络策略概述

### 2、入站、出站网络访问控制策略

