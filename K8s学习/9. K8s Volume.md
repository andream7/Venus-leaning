# K8s Volume

Kubernetes 的 Volume 支持多种类型，如下图所示：![2.png](https://cdn.nlark.com/yuque/0/2022/png/513185/1648538843251-3e1b92fa-f4a9-43ad-87e8-306789b291b3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

- Kubernetes 目前支持多达 28 种数据卷类型（其中大部分特定于具体的云环境如 GCE/AWS/Azure 等） 
  - 本地存储： emptyDir，HostPath
  - 网络连接性存储： SAN，NFS
  - 分布式存储：Glusterfs，CephFS
  - 云端存储：AzureFile，AzureDisk，AlibabaCloud



## 1、本地存储

### 1.1、EmptyDir

![EmptyDir概述.png](https://cdn.nlark.com/yuque/0/2021/png/513185/1610067269031-747b1f35-dd83-4397-b841-7ebb32b4b5d7.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_28%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。

EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且**无须指定宿主机上对应的目录文件**，因为kubernetes会自动分配一个目录，当Pod销毁时，EmptyDir中的数据也会被永久删除。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-emptydir
  namespace: dev
spec:
  containers:
    - name: busybox
      image: busybox:1.30
      volumeMounts:
        - name: logs-volume
          mountPath: /logs
  volumes:
    - name: logs-volume
      emptyDir: {}
```

### 1.2、HostPath

![HostPath概述.png](https://cdn.nlark.com/yuque/0/2021/png/513185/1610067328325-2d2b96b2-026f-48b5-a204-25333fba6984.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_32%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

EmptyDir中的数据不会被持久化，它会随着Pod的结束而销毁，如果想要简单的将数据持久化到主机中，可以选择HostPath。HostPath就是**将Node主机中的一个实际目录挂载到Pod中**，这样的设计就可以保证Pod销毁了，但是数据依旧可以保存在Node主机上。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-hostpath
  namespace: dev
spec:
  containers:
    - name: busybox
      image: busybox:1.30
      volumeMounts:
        - name: logs-volume
          mountPath: /logs
          subPath: cache1
  volumes:
    - name: logs-volume
      hostPath:
        path: /root/logs
        type: DirectoryOrCreate # 目录存在就使用，不存在就先创建再使用
        
# type的值的说明：
DirectoryOrCreate：目录存在就使用，不存在就先创建后使用。
Directory：目录必须存在。
FileOrCreate：文件存在就使用，不存在就先创建后使用。
File：文件必须存在。
Socket：unix套接字必须存在。
CharDevice：字符设备必须存在。
BlockDevice：块设备必须存在。
```

这里还有个 subPath，subPath 是什么？

先看一下，这两个容器都指定使用了同一个卷，就是这个 cache-volume。那么，在多个容器共享同一个卷的时候，为了隔离数据，我们可以通过 subPath 来完成这个操作。它会在卷里面建立两个子目录，然后容器 1 往 cache 下面写的数据其实都写在子目录 cache1 了，容器 2 往 cache 写的目录，其数据最终会落在这个卷里子目录下面的 cache2 下。

## 2、持久化存储

### 2.1、NFS

![NFS概述.png](https://cdn.nlark.com/yuque/0/2021/png/513185/1610067394717-c50b7ae8-41fd-4f6d-bd90-f9c27d0183a5.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_37%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-nfs
spec:
  containers:
    - name: busybox
      image: busybox:1.30
      volumeMounts:
        - name: logs-volume
          mountPath: /logs
  volumes:
    - name: logs-volume
      nfs:
        server: 192.168.18.100 # NFS服务器地址
        path: /root/data/nfs # 共享文件路径
```

### 2.2、CephFS

### 2.3、GlusterFS

## 3、Projected Volumes

Projected Volumes其实是将一些配置信息，如 secret/configmap 用卷的形式挂载在容器中，让容器中的程序可以通过 POSIX 接口来访问配置数据；

### 3.1、Secret

Secret 对象类型用来保存敏感信息，如：密码、OAuth2 令牌以及 SSH 密钥等。将这些信息放到 Secret 中比放在 Pod 的定义或者容器镜像中更加安全和灵活。

- 要使用 Secret，Pod 需要引用 Secret。 Pod 可以用三种方式之一来使用 Secret ： 

- - volume进行挂载

- - 作为[容器的环境变量]（envFrom字段引用）

- - 由 [kubelet 在为 Pod 拉取镜像时使用]（此时Secret是docker-registry类型的）

- 环境变量引用

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: my-secret
    namespace: default
  type: Opaque # Opaque 代表时用户定义的任意数据
  stringData: # stringData 意思是将数据编码交给 K8s，不用手动编码
    username: admin
    password: "1234556"
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-secret
  spec:
    containers:
    - name: alpine
      image: alpine
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
      env:
      - name: SECRET_USERNAME # 容器中的环境变量名称
        valueFrom:
          secretKeyRef: 
            name: my-secret
            key: username # secret 中 key 的名称，会自动 base64 解码
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: my-secret
            key: password
      - name: POD_NAME
        valueFrom: 
          fieldRef:  # 属性引用
            fieldPath: metadata.name
      - name: POD_LIMITS_MEMORY
        valueFrom:
          resourceFieldRef:  # 资源限制引用 
            containerName: alpine  
            resource: limits.memory
  ```

- 卷挂载使用

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: my-secret
    namespace: default
  type: Opaque
  stringData: 
    username: admin
    password: "1234556"
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: pod-secret
    namespace: default
    labels:
      app: pod-secret
  spec:
    containers:
    - name: alpine
      image: alpine
      volumeMounts:
      - name: app
        mountPath: /app  
    volumes:
      - name: app
        secret:
          secretName: my-secret # secret 的名称，Secret 中的所有 key 全部挂载出来
          items:
            - key: username # secret 中 key 的名称，Secret 中的 username 的内容挂载出来
              path: username.md # 在容器内挂载出来的文件的路径
  ```

  

### 3.2、Configmap

ConfigMap 和 Secret 非常类似，只不过 Secret 会将信息进行 base64 编码和解码，而 ConfigMap 却不会。

ConfigMap 和 Secret 一样，环境变量引用不会热更新，而卷挂载是可以热更新的。

## 2、PV与PVC

![PV和PVC概述.png](https://cdn.nlark.com/yuque/0/2021/png/513185/1610067446643-68eac8d4-685f-46d2-9ede-1d240f07f187.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

PVC相当于一个模板，里面写好业务需要的存储需求，PV是存储需求的实际承载体，通过controller-manager中的**PersisentVolumeController**将PVC与合适的PV bound到一起，从而满足用户的存储需求。

## 3、PV Spec 重要字段解析

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
	storageClassName: nfs-storage
  nfs: # 底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置有所不同。
    path: /root/data/pv1
    server: 192.168.18.100
  capacity:
    storage: 1Gi # 存储对象的大小
  accessModes: # 访问模式
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain # 回收策略
```



- **Capacity**：存储对象的大小；
- **AccessModes**：PV访问控制策略，一个PV可以设置多个访问策略
  - ReadWriteOnce：只允许单node访问
  - ReadOnlyMany：允许多个 node 只读访问
  - ReadWriteMany：允许多个 node 上读写访问

用户在提交 PVC 的时候，最重要的两个字段 —— Capacity 和 AccessModes。在提交 PVC后，K8s 集群中的相关组件是如何去找到合适的 PV 呢？首先它是通过为 PV 建立的 AccessModes 索引找到所有能够满足用户的 PVC 里面的 AccessModes 要求的 PV list，然后根据 PVC 的 Capacity，StorageClassName, Label Selector 进一步筛选 PV，如果满足条件的 PV 有多个，选择 PV 的 size 最小的，accessmodes 列表最短的 PV，也即最小适合原则。

- **PersistentReclaimPolicy**：PV被release之后的回收再利用策略
  - Recycle（已弃用）
  - Delete：PVC 被删除之后，PV 也会被删除
  - Retain（默认）：保留数据，需要管理员手动清理数据。
- **StorageClassName**：PVC可以通过该字段找到相同值的PV（静态供给），或者找到对应的StorageClass从而动态供给新的PV对象
- **NodeAffinity**：限制可以访问该volume的nodes

## 4、PV静态供应

由集群管理员预先创建一些 PV，然后用户在提交自己的存储需求（也就是 PVC）的时候，K8s 内部相关组件会把 PVC 和 PV 做绑定；之后用户再通过 pod 去使用存储的时候，就可以通过 PVC 找到相应的 PV，它就可以使用了。

静态产生方式有什么不足呢？

可以看到，预分配其实是很难预测用户真实需求的。举一个最简单的例子：如果用户需要的是 20G，然而集群管理员在分配的时候可能有 80G 、100G 的，但没有 20G 的，这样就很难满足用户的真实需求，也会造成资源浪费。



1. 管理员：

   1. 通过阿里云文件存储控制台，创建 NAS 文件系统和添加挂载点，

   1. 创建 PV 对象中，将NAS 文件系统大小，挂载点，将access mode，reclaim policy等信息添加到PV对象。

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nas-csi-pv
spec:
  capacity:
  	storage: 5Gi  # 该pv的总容量大小
  accessModes:
   - ReadWriteMany # 使用该pv的pod都具有读写权限
  persistentVolumeReclaimPolicy: Retain # 该pv的回收策略
  csi:
  	driver: nasplugin.csi.alibabacloud.com # 指定volume plugin
  	volumeHandle: data-id
  	volumeAttributes: 
  		host: 192.168.200.171
  		path: "/k8s"
  		vers: "4.0"
```

2. 用户创建 PVC 和 pod

```yaml
# 用户创建pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nas-pvc
spec:
  accessModes:
   - ReadWriteMany 
  resources:
  	requests:
  		storage: 5Gi
  		
# 用户创建Pod
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: nas-pvc
      mountPath: /data
  volumes:
  - name: nas-pvc
    PersistentVolumeClaim:
     claimName: nas-pvc
```

上面pvc的 size 和它的access mode，跟我们刚才静态创建这块 PV 其实是匹配的。

这样的话，当用户在提交 PVC 的时候，K8s 集群相关的组件就会把 PV 的 PVC bound 到一起。之后，用户在提交 pod yaml 的时候，可以在卷里面写上 PVC 声明，在 PVC 声明里面可以通过 claimName 来声明要用哪个 PVC。

这时，挂载方式其实跟前面讲的一样，当提交完 yaml 的时候，它可以通过 PVC 找到 bound 着的那个 PV，然后就可以用那块存储了。这是静态 Provisioning 到被 pod 使用的一个过程。

## 5、PV 动态供给（StorageClass）

![41.png](https://cdn.nlark.com/yuque/0/2022/png/513185/1648539197723-ab90990e-7dab-492e-94ec-4e37d33583b4.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_48%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fresize%2Cw_1500%2Climit_0)

动态供给是什么意思呢？

就是说现在集群管理员不预分配 PV，他写了一个模板文件，这个模板文件是用来表示创建不同类型存储（块存储，文件存储等）的模板，用户在PVC中指定使用哪种存储模板（StorageClass）以及自己需要的大小、访问方式等参数，K8s自动生成相应的PV对象。

K8s 集群中的管控组件，会结合 PVC 和 StorageClass 的信息动态，生成用户所需要的存储（PV），将 PVC 和 PV 进行绑定后，pod 就可以使用 PV 了。

1. 管理员创建StorageClass

   ```yaml
   apiVersion: storage.k8s.io/v1
   kind: StorageClass
   metadata:
   	name: csi-disk
   provisioner: diskplugin.csi.alibabacloud.com # 动态供给需要相应存储插件，这个插件需要部署到K8s集群中
   parameters:
   	regionId: cn-Beijing
   	zoneId: cn-beijing-b
   	fsType: ext4
   	type: cloud_ssd
   reclaimPolicy: Delete # pv的回收策略
   ```

2. 用户创建PVC和Pod

   PVC 需要新加一个字段 StorageClassName，它的意思是指定动态创建 PV 的模板文件的名字，这里 StorageClassName 填的就是上面声明的 csi-disk。

   在提交完 PVC之后，K8s 集群中的相关组件就会根据 PVC 以及对应的 StorageClass 动态生成这块 PV 给这个 PVC 做一个绑定，之后用户在提交自己的 yaml 时，用法和接下来的流程和前面的静态使用方式是一样的，通过 PVC 找到我们动态创建的 PV，然后把它挂载到相应的容器中就可以使用了。

   ```yaml
   # 用户创建pvc
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: my-pvc
   spec:
     accessModes:
       - ReadWriteOnce
     storageClassName: csi-disk # 指定使用的存储类
     resources:
       requests:
         storage: 10Gi # 请求的存储容量
   
     		
   # 用户创建Pod
   apiVersion: v1
   kind: Pod
   metadata:
     name: example-pod
   spec:
     containers:
     - name: nginx
       image: nginx:latest
       volumeMounts:
       - name: nas-pvc
         mountPath: /data
     volumes:
     - name: nas-pvc
       PersistentVolumeClaim:
        claimName: nas-pvc
   ```

   

## 6、架构设计

csi 是什么？csi 的全称是 container storage interface，它是 K8s 社区后面对存储插件实现 ( out of tree ) 的官方推荐方式。csi 的实现大体可以分为两部分：

- 第一部分是由 k8s 社区驱动实现的通用的部分，像我们这张图中的 csi-provisioner和 csi-attacher；
- 另外一种是由云存储厂商实践的，对接云存储厂商的 OpenApi，主要是实现真正的 create/delete/mount/unmount 存储的相关操作，对应到 csi-controller-server 和 csi-node-server。

1. 用户提交 pvc.yaml 之后，k8s 内部的处理流程
   1. 第一阶段：用户在提交 pvc.yaml 的时候，首先会在集群中生成一个 PVC 对象，然后 PVC 对象会被 csi-provisioner 监听到，csi-provisioner 会结合 PVC 对象以及 PVC 对象中声明的 storageClass，通过 GRPC 调用 csi-controller-server，然后，到云存储服务这边去创建真正的存储，并最终创建出来 PV 对象。最后，由集群中的 PV controller 将 PVC 和 PV 对象做 bound 之后，这个 PV 就可以被使用了。
   2. 第二阶段：之后用户在提交 pod.yaml 的时候，首先会被调度选中某一个合适的 node，等 pod 的运行 node 被选出来之后，会被 AD Controller watch 到 pod 选中的 node，它会去查找 pod 中使用了哪些 PV。然后它会生成一个内部的对象叫 VolumeAttachment 对象，从而去触发 csi-attacher去调用 csi-controller-server 去做真正的 attache 操作，attach 操作调到云存储厂商 OpenAPI。这个 attach 操作就是将存储 attach到 pod 将会运行的 node 上面。
   3. 第三阶段。第三阶段发生在 kubelet 创建 pod 的过程中，它在创建 pod 的过程中，首先要去做一个 mount，这里的 mount 操作是为了将已经 attach 到这个 node 上面那块盘，进一步 mount 到 pod 可以使用的一个具体路径，之后 kubelet 才开始创建并启动容器。这就是 PV 加 PVC 创建存储以及使用存储的第三个阶段 —— mount 阶段。

总的来说，有三个阶段：第一个 create 阶段，主要是创建存储；第二个 attach 阶段，就是将那块存储挂载到 node 上面(通常为将存储 load 到 node 的 /dev 下面)；第三个 mount 阶段，将对应的存储进一步挂载到 pod 可以使用的路径。这就是我们的 PVC、PV、已经通过 CSI 实现的卷从创建到使用的完整流程。

## 7、生命周期

![PVC的生命周期.png](https://cdn.nlark.com/yuque/0/2021/png/513185/1610067602846-ec97175f-5f8d-47f2-9f93-9fabb4621f0d.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_35%2Ctext_6K645aSn5LuZ%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

1. 资源供应：PV被创建出来等待使用

2. 资源绑定：
   1. 用户定义好PVC之后，系统将根据PVC对存储资源的请求在以存在的PV中选择一个满足条件的。
   2. 一旦找到，就将该PV和用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了。
   3. 如果找不到，PVC就会无限期的处于Pending状态，直到系统管理员创建一个符合其要求的PV。
   4. PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再和其他的PVC进行绑定了。


3. 资源使用：用户可以在Pod中像volume一样使用PVC，将PVC挂载到容器内的某个路径进行使用。

4. 资源释放：当存储资源使用完毕后，用户可以删除PVC，和该PVC绑定的PV将会标记为“已释放”，但是还不能立刻和其他的PVC进行绑定。通过之前PVC写入的数据可能还留在存储设备上，只有在清除之后该PV才能再次使用。


5. 资源回收：K8s根据PV设置的回收策略进行资源的回收。

